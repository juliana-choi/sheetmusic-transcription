{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CPEN291_FinalProject.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwT9gODeb5u6"
      },
      "source": [
        "# CPEN 291 Final Project\n",
        "\n",
        "#Proposal:\n",
        "Our project would involve turning youtube videos of piano compositions into sheet music. As a person who loves to learn new songs through youtube, I found that it can be often expensive to buy sheet music and instead I try to learn by slowing down the video and looking at the artist's hands. However, that has proven itself as quite a challenge since many times the pianist might edit the video in a way that their fingers aren't visible during all the performance. As a solution, I thought it would be interesting to build an ML system that would take audio as an input (mostly from videos on youtube and other streaming platforms) and create music sheets that would be readily available for download. Upon research, I found that there are quite a few companies that offer similar services, which makes me hopeful of the possibility of implementing my project. There are quite a few articles that mention new exciting ML algorithms such as magenta and Deep Watershed Detection. I would probably rely on a scraping algorithm to get a database with audios from amazing piano performances posted on all sorts of social media. Then, I would sort out the database by converting the audio into something more intelligible (such as waves) that would be analyzed by my model. The model would use the data from the waves to point out what note that is, what's its length, etc. In order to train the model, I could find cheap or free sheet music online in order to have a comparison of what the model generated to what it is originally supposed to look like. Finally, once having an acceptable accuracy rate, I would make the sheet music available on a website where we would be able to download it and learn from it. (Note that this would serve mostly as practice for beginners/intermediate students, as it would not be 100% correct)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oih6ZQXmlOyd"
      },
      "source": [
        "#Import Statements\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAu-2zJZQofM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45e9caff-f81d-4c6e-d433-43ad7e088225"
      },
      "source": [
        "!pip install pretty_midi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pretty_midi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/8e/63c6e39a7a64623a9cd6aec530070c70827f6f8f40deec938f323d7b1e15/pretty_midi-0.2.9.tar.gz (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from pretty_midi) (1.19.5)\n",
            "Collecting mido>=1.1.16\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/0a/81beb587b1ae832ea6a1901dc7c6faa380e8dd154e0a862f0a9f3d2afab9/mido-1.2.9-py2.py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pretty_midi) (1.15.0)\n",
            "Building wheels for collected packages: pretty-midi\n",
            "  Building wheel for pretty-midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty-midi: filename=pretty_midi-0.2.9-cp37-none-any.whl size=5591954 sha256=a22af4a31c7a5918957ab615eed7884d11fc24f27e62532af692f3e1ea7f713d\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/a1/c6/b5697841db1112c6e5866d75a6b6bf1bef73b874782556ba66\n",
            "Successfully built pretty-midi\n",
            "Installing collected packages: mido, pretty-midi\n",
            "Successfully installed mido-1.2.9 pretty-midi-0.2.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xtlGKFolQ47"
      },
      "source": [
        "import pandas as pd, csv\n",
        "import torch, torchtext\n",
        "from torch import nn, optim, functional as F\n",
        "from torchvision import datasets, models, transforms\n",
        "from tqdm.auto import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pretty_midi\n",
        "import librosa\n",
        "from librosa import display\n",
        "from IPython.display import Audio,display\n",
        "import os\n",
        "from scipy.io import wavfile\n",
        "import PIL\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8U1GWH9lvvn"
      },
      "source": [
        "!pip install pydub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDFPT5Bul5fi"
      },
      "source": [
        "from pydub import AudioSegment\n",
        "song = AudioSegment.from_mp3(\"/swallows.mp3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qRQ6DOYoSXu"
      },
      "source": [
        "Here I am testing some stuff out with the AudioSegment import, refer to: https://github.com/jiaaro/pydub#installation\n",
        "\n",
        "-> NOTE: It seems like .mid files cannot be processed  :("
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nAxJ-YErQLj"
      },
      "source": [
        "# Testing of Different Approaches/Ideas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tBkHUpHoQnG"
      },
      "source": [
        "# Size of segments to break song into for volume calculations\n",
        "SEGMENT_MS = 50\n",
        "# dBFS is decibels relative to the maximum possible loudness\n",
        "volume = [segment.dBFS for segment in song[::SEGMENT_MS]]\n",
        "x_axis = np.arange(len(volume)) * (SEGMENT_MS / 1000)\n",
        "plt.plot(x_axis, volume)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uZdw7pyom1J"
      },
      "source": [
        "# Minimum volume necessary to be considered a note\n",
        "VOLUME_THRESHOLD = -35\n",
        "# The increase from one sample to the next required \n",
        "# to be considered a note\n",
        "EDGE_THRESHOLD = 5\n",
        "predicted_starts = []\n",
        "for i in range(1, len(volume)):\n",
        "    if (\n",
        "        volume[i] > VOLUME_THRESHOLD and \n",
        "        volume[i] - volume[i - 1] > EDGE_THRESHOLD\n",
        "    ):\n",
        "        ms = i * SEGMENT_MS\n",
        "        predicted_starts.append(ms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNjiY2TUpDM5"
      },
      "source": [
        "# Throw out any additional notes found in this window\n",
        "MIN_MS_BETWEEN = 100\n",
        "predicted_starts = []\n",
        "for i in range(1, len(volume)):\n",
        "    if (\n",
        "        volume[i] > VOLUME_THRESHOLD and \n",
        "        volume[i] - volume[i - 1] > EDGE_THRESHOLD\n",
        "    ):\n",
        "        ms = i * SEGMENT_MS\n",
        "        # Ignore any too close together\n",
        "        if (\n",
        "            len(predicted_starts) == 0 or\n",
        "            ms - predicted_starts[-1] >= MIN_MS_BETWEEN\n",
        "        ):\n",
        "            predicted_starts.append(ms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH0_QA-8pJVq"
      },
      "source": [
        "def frequency_spectrum(sample, max_frequency=800):\n",
        "    \"\"\"\n",
        "    Derive frequency spectrum of a pydub.AudioSample\n",
        "    Returns an array of frequencies and an array of how prevalent that frequency is in the sample\n",
        "    \"\"\"\n",
        "    \n",
        "    # Convert pydub.AudioSample to raw audio data\n",
        "    # Copied from Jiaaro's answer on https://stackoverflow.com/questions/32373996/pydub-raw-audio-data\n",
        "    bit_depth = sample.sample_width * 8\n",
        "    array_type = get_array_type(bit_depth)\n",
        "    raw_audio_data = array.array(array_type, sample._data)\n",
        "    n = len(raw_audio_data)\n",
        "    # Compute FFT and frequency value for each index in FFT array\n",
        "    # Inspired by Reveille's answer on https://stackoverflow.com/questions/53308674/audio-frequencies-in-python\n",
        "    freq_array = np.arange(n) * (float(sample.frame_rate) / n)  # two sides frequency range\n",
        "    freq_array = freq_array[:(n // 2)]  # one side frequency range\n",
        "    raw_audio_data = raw_audio_data - np.average(raw_audio_data)  # zero-centering\n",
        "    \n",
        "    freq_magnitude = scipy.fft(raw_audio_data) # fft computing and normalization\n",
        "    freq_magnitude = freq_magnitude[:(n // 2)] # one side\n",
        "    if max_frequency:\n",
        "        max_index = int(max_frequency * n / sample.frame_rate) + 1\n",
        "        freq_array = freq_array[:max_index]\n",
        "        freq_magnitude = freq_magnitude[:max_index]\n",
        "    freq_magnitude = abs(freq_magnitude)\n",
        "    freq_magnitude = freq_magnitude / np.sum(freq_magnitude)\n",
        "    return freq_array, freq_magnitude"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCH8pcjtptPX"
      },
      "source": [
        "freq_array, freq_magnitude = frequency_spectrum(song, 800)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTnnc5PDpKnU"
      },
      "source": [
        "peak_indicies, props = scipy.signal.find_peaks(freq_magnitudes, height=0.015)\n",
        "for i, peak in enumerate(peak_indicies):\n",
        "    freq = freq_array[peak]\n",
        "    magnitude = props[\"peak_heights\"][i]\n",
        "    print(\"{}hz with magnitude {:.3f}\".format(freq, magnitude))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcktfBl9pqOw"
      },
      "source": [
        "get_note_for_freq(freq_array[np.argmax(freq_magnitude)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zym_FyK1raTy"
      },
      "source": [
        "# code adapted from https://github.com/jsleep/wav2mid/blob/master/notebooks/wavmidi_preprocess.ipynb\n",
        "# This code converts .mid files to spectrograms\n",
        "\n",
        "# Note: the below code requires you to manually install a new version of fluidsynth (using pip install gives you\n",
        "# a version that is too old). This code must also be run on a 32-bit version of Python. If memory size errors occur,\n",
        "# you must increase page size in your devices settings.\n",
        "\n",
        "import fluidsynth\n",
        "\n",
        "PATH_TO_ENTRIES = ''\n",
        "DIR_SAVE = ''\n",
        "\n",
        "entries = os.listdir(PATH_TO_ENTRIES)\n",
        "\n",
        "for entry in entries:\n",
        "    \n",
        "    midi_fn = PATH_TO_ENTRIES + entry\n",
        "    sr = 22050\n",
        "\n",
        "    pm = pretty_midi.PrettyMIDI(midi_fn)\n",
        "\n",
        "    y = pm.fluidsynth(fs=sr)[:sr*5]\n",
        "    D = librosa.stft(y)\n",
        "    librosa.display.specshow(librosa.amplitude_to_db(D,ref=np.max),y_axis='log', x_axis='time', sr=sr)\n",
        "    plt.title('Power spectrogram')\n",
        "\n",
        "    plt.savefig(DIR_SAVE +  '/' + entry.replace('.mid', '.png'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n3B12hajHbf"
      },
      "source": [
        "# Dataset Collection\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4sTLGq4jKe2"
      },
      "source": [
        "# Code to scrape samples using Selenium. Requires user to install chromedriver.\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import time\n",
        "\n",
        "PATH_TO_CHROMEDRIVER = ''\n",
        "\n",
        "driver = webdriver.Chrome(PATH_TO_CHROMEDRIVER)\n",
        "\n",
        "driver.get('https://www.mutopiaproject.org/cgibin/make-table.cgi?Instrument=Piano')\n",
        "\n",
        "for i in range(77):\n",
        "     try:\n",
        "        tables_mid = []\n",
        "        tables_ps = []\n",
        "\n",
        "        for j in range(1,11):\n",
        "            tables_mid.append(driver.find_element_by_xpath(f\"//table/tbody//tr[{j}]/td//table//tbody/tr[4]/td[2]\"))\n",
        "            tables_ps.append(driver.find_element_by_xpath(f\"//table/tbody//tr[{j}]/td//table//tbody/tr[5]/t'd\"))\n",
        "\n",
        "        for table in tables_mid:\n",
        "            time.sleep(0.5)\n",
        "            table.find_element_by_partial_link_text('.mid').click()\n",
        "\n",
        "        for table in tables_ps:\n",
        "            time.sleep(1)\n",
        "            driver.execute_script(\"arguments[0].click();\", table.find_element_by_partial_link_text('.ps'))\n",
        "\n",
        "        link = driver.find_element_by_link_text('Next 10')\n",
        "        link.click()\n",
        "\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "\n",
        "driver.quit()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyW90BNjidEQ"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuemryEmvHBD"
      },
      "source": [
        "To convert sheets from .ps format to .pdf, Ghostscript, an external application was used. [This](https://stackoverflow.com/questions/44532739/using-ghostscript-in-a-windows-bat-file-to-convert-multiple-pdf-files-to-png) Stackoverflow post below gave a command that, with slight modification, allowed us to batch-convert these files to pdf format using Ghostscript. Below is how we then converted the pdf files to png."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnVK4NXcuhHv"
      },
      "source": [
        "# Code to convert sheets from pdf to png format\n",
        "\n",
        "from pdf2image import convert_from_path\n",
        "PATH_SHEETS_PDF = ''\n",
        "PATH_SHEETS_PNG = ''\n",
        "entries = os.listdir(PATH_SHEETS_PDF)\n",
        "\n",
        "for entry in entries:\n",
        "    images = convert_from_path(PATH_SHEETS_PDF + '/' + entry)\n",
        "    entry = entry.replace('.pdf', '')\n",
        "\n",
        "    for i in range(len(images)):\n",
        "        images[i].save(PATH_SHEETS_PNG + entry + f'_{i}.png', 'PNG')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ekfj9pRioYc"
      },
      "source": [
        "The following code was used to convert any audio files from stereo to mono"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIup2XEdiveS"
      },
      "source": [
        "# from https://stackoverflow.com/questions/5120555/how-can-i-convert-a-wav-from-stereo-to-mono-in-python\n",
        "\n",
        "for entry in DIR_WAV:\n",
        "    sound = AudioSegment.from_wav(PATH_WAV + '/' + entry)\n",
        "    sound = sound.set_channels(1)\n",
        "    sound.export(PATH_WAV + '/' + entry, format='wav')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWVPFZxoj9V7"
      },
      "source": [
        "The following helper function is used to create 2D matrix labels from the midi file of a sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVv8haYG_dbv"
      },
      "source": [
        "# from https://github.com/jsleep/wav2mid/blob/master/examples/one_hot.py\n",
        "\n",
        "from __future__ import division\n",
        "\"\"\"\n",
        "Simple function for converting Pretty MIDI object into one-hot encoding\n",
        "/ piano-roll-like to be used for machine learning.\n",
        "\"\"\"\n",
        "\n",
        "def get_label(pm, fs=1):\n",
        "    \"\"\"Compute a one hot matrix of a pretty midi object\n",
        "    Parameters\n",
        "    ----------\n",
        "    pm : pretty_midi.PrettyMIDI\n",
        "        A pretty_midi.PrettyMIDI class instance describing\n",
        "        the piano roll.\n",
        "    fs : int\n",
        "        Sampling frequency of the columns, i.e. each column is spaced apart\n",
        "        by ``1./fs`` seconds.\n",
        "    Returns\n",
        "    -------\n",
        "    one_hot : np.ndarray, shape=(128,times.shape[0])\n",
        "        Piano roll of this instrument. 1 represents Note Ons,\n",
        "        -1 represents Note offs, 0 represents constant/do-nothing\n",
        "    \"\"\"\n",
        "\n",
        "    # Allocate a matrix of zeros - we will add in as we go\n",
        "    one_hots = []\n",
        "\n",
        "    for instrument in pm.instruments:\n",
        "        one_hot = np.zeros((128, int(fs*instrument.get_end_time())+1))\n",
        "        for note in instrument.notes:\n",
        "            # note on\n",
        "            one_hot[note.pitch, int(note.start*fs)] = 1          # Losing precision with these casts. Try to fix? (use time windows)\n",
        "            # print('note on',note.pitch, int(note.start*fs))\n",
        "            # note off\n",
        "            one_hot[note.pitch, int(note.end*fs)] = -1\n",
        "            # print('note off',note.pitch, int(note.end*fs))\n",
        "        one_hots.append(one_hot)\n",
        "\n",
        "    one_hot = np.zeros((128, np.max([o.shape[1] for o in one_hots])))\n",
        "    for o in one_hots:\n",
        "        one_hot[:, :o.shape[1]] += o\n",
        "\n",
        "    one_hot = np.clip(one_hot,-1,1)\n",
        "    print(one_hot.shape)\n",
        "    return torch.as_tensor(one_hot)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH4c5m2SkN27"
      },
      "source": [
        "The following helper function applies stft (or cqt) to the signal data of an audio sample and saves the resulting spectrogram. It also transforms the image into a tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmH29ha7kMc0"
      },
      "source": [
        "def get_sample(signalData, i, transform):\n",
        "    signalData_float = signalData.astype(float)\n",
        "    f = librosa.stft(signalData_float)\n",
        "    librosa.display.specshow(librosa.amplitude_to_db(f, ref=np.max), y_axis='log', x_axis='time', sr=22050)\n",
        "    plt.savefig(f'image_spec_{i}.jpg')\n",
        "    img = PIL.Image.open(f'image_spec_{i}.jpg')\n",
        "    img = transform(img)\n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB9ubDFOkdPH"
      },
      "source": [
        "The function below creates a dataset where samples consist of images of the spectrogram representation of a song and labels that are 2D arrays where each row represents a second of the piece of \n",
        "music and each column represents a certain note. So if the sample is 90 seconds long, the label will be a matrix with 90 rows and 128 columns since there are 128 midi notes.\n",
        "\n",
        "If note j is played at time i, label[i][j] = 1. \n",
        "\n",
        "If this note becomes \"off\" at time k, label[k][j] = -1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FQmvUihgJSY"
      },
      "source": [
        "def create_dataset(path_mid, path_wav, transform):\n",
        "    entries_mid = os.listdir(path_mid)\n",
        "    entries_wav = os.listdir(path_wav)\n",
        "    dataset = []\n",
        "\n",
        "    for i in range(len(entries_wav)):\n",
        "        # print(\"itr: \" + str(i))\n",
        "        pm = pretty_midi.PrettyMIDI(path_mid + '/' + entries_mid[i])\n",
        "        samplingFrequency, signalData = wavfile.read(path_wav + '/' + entries_wav[i])\n",
        "        sample = get_sample(signalData, i, transform)\n",
        "        label = get_label(pm)\n",
        "        dataset.append((sample, label))\n",
        "        \n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLVDmRoFQznM"
      },
      "source": [
        "class Dataset():\n",
        "  def __init__(self, PATH_MID, PATH_WAV):\n",
        "    transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n",
        "    self.dataset = create_dataset(PATH_MID, PATH_WAV, transform)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.dataset)\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    if torch.is_tensor(i):\n",
        "      i = i.item()\n",
        "\n",
        "    return self.dataset[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16XGrVhvjK4f"
      },
      "source": [
        "# Model, Training and Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zC5fl2htkL2V"
      },
      "source": [
        "The model will receive the input as being an audio file which will be analyzed using some python libraries we have found that can be useful to plot the data we have in the waveform type. We also already know how to convert them into Spectograms which can also be useful to analyze data. Our model will output notes (i.e.: ['B', 'C#', ..., 'G'])."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOr7isnujPVu"
      },
      "source": [
        "# def toSpectrogram(entries):\n",
        "#   for entry in entries:\n",
        "    \n",
        "#     midi_fn = PATH_TO_ENTRIES + entry\n",
        "#     sr = 22050\n",
        "\n",
        "#     pm = pretty_midi.PrettyMIDI(midi_fn)\n",
        "\n",
        "#     y = pm.fluidsynth(fs=sr)[:sr*5]\n",
        "#     D = librosa.stft(y, n_fft=512) # change n_fft to change how much of the song you capture\n",
        "#     librosa.display.specshow(librosa.amplitude_to_db(D,ref=np.max),y_axis='log', x_axis='time', sr=sr)\n",
        "#     plt.title('Power spectrogram')\n",
        "\n",
        "#     plt.savefig(DIR_SAVE + entry.replace('.mid', '.png'))\n",
        "\n",
        "# dataset = toSpectrogram(midi_files) #to change to where midi files are saved\n",
        "# n_train = int(0.8 * len(dataset))\n",
        "# n_test = len(dataset) - n_train\n",
        "# rng = torch.Generator().manual_seed(0)\n",
        "# train, test = torch.utils.data.random_split(dataset, [n_train, n_test], rng)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkQ-vw5soi1v"
      },
      "source": [
        "# def run_train(model, ds, crit, opt, dev, n_epochs=10, batch_sz=128):\n",
        "#     model = model.to(dev)\n",
        "#     model.train()\n",
        "#     ldr = torch.utils.data.DataLoader(ds, batch_size=batch_sz)\n",
        "\n",
        "#     for i in range(n_epochs):\n",
        "#       for samples in ds:\n",
        "#           model.zero_grad()\n",
        "#           # batch_sz = samples[0].size(0)\n",
        "                  \n",
        "#           samples = samples[0].to(dev)\n",
        "#           # labels = torch.full(1, dtype=torch.float, device=dev)\n",
        "#           outs = model(samples).squeeze()\n",
        "#           loss = crit(outs, label)\n",
        "#           loss.backward()\n",
        "\n",
        "#           opt.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rknWis8kq2k"
      },
      "source": [
        "# Conclusion\n",
        "Given the output notes we will now try to render each of our notes into pictures and assemble them as sheet music."
      ]
    }
  ]
}